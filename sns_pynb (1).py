# -*- coding: utf-8 -*-
"""sns_pynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oDbVyNFUAaB13fGlXKowbwtj41fsbESH

Preprocessing For Twitter.csv
"""

# All imports
import pandas as pd
import ast
import re
import torch
import torch.nn as nn
import numpy as np
import networkx as nx
import random
# Install PyTorch Geometric and dependencies
!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cu118.html
!pip install torch-geometric

!pip install torch_geometric
!pip install scikit-learn

# STEP 1: Install pyvis
!pip install pyvis

from torch_geometric.data import HeteroData
from torch_geometric.nn import SAGEConv, HeteroConv
from torch_geometric.transforms import ToUndirected
from pyvis.network import Network

# Load Twitter dataset
twitter_df = pd.read_csv("/content/India.csv", engine='python')

# Keep only relevant columns
twitter_df = twitter_df[['user_id', 'username', 'hashtags','language']]

# Safely parse and clean hashtags
def safe_parse_hashtags(val):
    try:
        if isinstance(val, str):
            val = ast.literal_eval(val)
        if not isinstance(val, list):
            return []
        return [tag for tag in val if isinstance(tag, (str, int, float))]
    except:
        return []

def clean_tag(tag):
    result = ''
    for c in str(tag).strip('#'):
        if c.isalpha():
            result += c.lower()  # convert letters to lowercase
        else:
            result += c  # keep numbers and symbols unchanged
    return result

twitter_df['hashtags'] = twitter_df['hashtags'].apply(safe_parse_hashtags)
twitter_df['hashtags'] = twitter_df['hashtags'].apply(lambda tags: [clean_tag(tag) for tag in tags if tag])

# Filter only English and Hindi tweets
twitter_df = twitter_df[twitter_df['language'].isin(['en', 'in'])]

# Drop the language column
twitter_df = twitter_df.drop(columns=['language'])

# STEP 8: Reset index
twitter_df = twitter_df.reset_index(drop=True)

# STEP 9: Preview the cleaned dataset
twitter_df

"""Preprocessing for Github.csv"""

# Load GitHub repo dataset
github_df = pd.read_csv("/content/repository.csv")

# Drop rows with missing essential values
github_df = github_df.dropna(subset=['Name', 'Topics', 'URL'])

# Keep only useful columns
github_df = github_df[['Name','Topics','URL']]

# Drop duplicates (based on repo name or URL)
github_df = github_df.drop_duplicates(subset=['Name', 'URL'])

# Limit to first 20,000 entries
github_df = github_df.head(20000)

# Reset index
github_df = github_df.reset_index(drop=True)

# Preview the cleaned dataset
github_df

twitter_df.to_csv("/content/twitter_cleaned.csv", index=False)
github_df.to_csv("/content/github_cleaned.csv", index=False)

"""Graph creation"""

# Create unique tweet identifiers (user_id + tweet_index)
twitter_df['tweet_id'] = twitter_df.groupby('user_id').cumcount().astype(str)
twitter_df['unique_tweet_id'] = twitter_df['user_id'].astype(str) + '_tweet_' + twitter_df['tweet_id']

# Use unique tweet IDs instead of just user IDs
twitter_tweets = twitter_df['unique_tweet_id'].tolist()  # Each tweet gets separate ID
hashtags = list(set([tag for tags in twitter_df['hashtags'] for tag in tags]))
repos = github_df['Name'].unique().tolist()

# Handle Topics column safely
github_df['Topics'] = github_df['Topics'].fillna('[]').apply(ast.literal_eval)
topics = list(set([topic.lower().strip() for sublist in github_df['Topics'] for topic in sublist]))

# Create node ID maps - Modified for tweet-level granularity
tweet2id = {tid: i for i, tid in enumerate(twitter_tweets)}  # Maps each tweet to unique ID
hashtag2id = {h: i for i, h in enumerate(hashtags)}
repo2id = {r: i for i, r in enumerate(repos)}
topic2id = {t: i for i, t in enumerate(topics)}

# Keep user mapping for final predictions
user2tweets = twitter_df.groupby('user_id')['unique_tweet_id'].apply(list).to_dict()

# Build edges for Tweet → Hashtag connections
edges_tweet_hashtag = []
for _, row in twitter_df.iterrows():
    tweet_id = row['unique_tweet_id']
    if tweet_id not in tweet2id:
        continue

    tweet_hashtags = row['hashtags']
    for tag in tweet_hashtags:
        if tag in hashtag2id:
            edges_tweet_hashtag.append([tweet2id[tweet_id], hashtag2id[tag]])

print(f"Created {len(edges_tweet_hashtag)} tweet-hashtag connections")

# GitHubRepo → Topic
edges_repo_topic = []
for _, row in github_df.iterrows():
    repo = row['Name']
    if repo not in repo2id:
        continue
    for topic in row['Topics']:
        t = topic.lower().strip()
        if t in topic2id:
            edges_repo_topic.append([repo2id[repo], topic2id[t]])

# Hashtag → Topic (if a hashtag matches a topic string)
edges_hashtag_topic = []
for h in hashtags:
    if h in topic2id:
        edges_hashtag_topic.append([hashtag2id[h], topic2id[h]])

# Build HeteroData graph - Modified for tweet-level nodes
data = HeteroData()

data['Tweet'].num_nodes = len(twitter_tweets)  # Changed from TwitterUser to Tweet
data['Hashtag'].num_nodes = len(hashtags)
data['Topic'].num_nodes = len(topics)
data['GitHubRepo'].num_nodes = len(repos)

hidden_dim = 128  # or 64

data['Tweet'].x = torch.randn(data['Tweet'].num_nodes, hidden_dim)  # Tweet features
data['Hashtag'].x = torch.randn(data['Hashtag'].num_nodes, hidden_dim)
data['Topic'].x = torch.randn(data['Topic'].num_nodes, hidden_dim)
data['GitHubRepo'].x = torch.randn(data['GitHubRepo'].num_nodes, hidden_dim)

data['Tweet', 'uses', 'Hashtag'].edge_index = torch.tensor(edges_tweet_hashtag, dtype=torch.long).t().contiguous()
data['GitHubRepo', 'has_topic', 'Topic'].edge_index = torch.tensor(edges_repo_topic, dtype=torch.long).t().contiguous()
data['Hashtag', 'related_to', 'Topic'].edge_index = torch.tensor(edges_hashtag_topic, dtype=torch.long).t().contiguous()

print(data)

"""Graph visualization"""

# Create a NetworkX subgraph from the full heterogeneous data
G = nx.Graph()

# Sampling 300 Tweets randomly (from all tweets)
sample_tweets = random.sample(twitter_tweets, min(300, len(twitter_tweets)))

# Add nodes and edges for each sampled tweet
for tweet_id in sample_tweets:
    tweet_node = f"tw_{tweet_id}"
    # Get original user info for display
    user_info = twitter_df[twitter_df['unique_tweet_id'] == tweet_id].iloc[0]
    display_label = f"{user_info['username']}_tweet_{user_info['tweet_id']}"
    G.add_node(tweet_node, label=display_label, title="Tweet", color='skyblue')

    # Add hashtags used by this tweet
    if tweet_id in tweet2id:
        tweet_idx = tweet2id[tweet_id]
        tags = [tgt for src, tgt in edges_tweet_hashtag if src == tweet_idx]
        for tag_idx in tags:
            tag = hashtags[tag_idx]
            tag_node = f"ht_{tag}"
            G.add_node(tag_node, label=tag, title="Hashtag", color='orange')
            G.add_edge(tweet_node, tag_node, label="uses")

            # Add related topic if available
            if tag in topic2id:
                topic_idx = topic2id[tag]
                topic = tag  # same as tag name
                topic_node = f"tp_{topic}"
                G.add_node(topic_node, label=topic, title="Topic", color='lightgreen')
                G.add_edge(tag_node, topic_node, label="related_to")

                # Add GitHub repos with that topic
                for src, tgt in edges_repo_topic:
                    if tgt == topic_idx:
                        repo_name = repos[src]
                        repo_node = f"gh_{repo_name}"
                        G.add_node(repo_node, label=repo_name, title="GitHubRepo", color='pink')
                        G.add_edge(repo_node, topic_node, label="has_topic")

net = Network(
    notebook=True,
    height="800px",
    width="100%",
    bgcolor="#222222",
    font_color="white",
    cdn_resources='remote'
)

net.from_nx(G)
net.show("hetero_graph.html")
from google.colab import files
files.download('hetero_graph.html')

"""Machine learning Model Training"""

# Convert heterogeneous graph to undirected so all nodes get updated
data = ToUndirected()(data)

class HeteroGNN(nn.Module):
    def __init__(self, hidden_dim=128):
        super().__init__()
        self.conv1 = HeteroConv({
            ('Tweet', 'uses', 'Hashtag'): SAGEConv((-1, -1), hidden_dim),
            ('Hashtag', 'rev_uses', 'Tweet'): SAGEConv((-1, -1), hidden_dim),
            ('Hashtag', 'related_to', 'Topic'): SAGEConv((-1, -1), hidden_dim),
            ('Topic', 'rev_related_to', 'Hashtag'): SAGEConv((-1, -1), hidden_dim),
            ('GitHubRepo', 'has_topic', 'Topic'): SAGEConv((-1, -1), hidden_dim),
            ('Topic', 'rev_has_topic', 'GitHubRepo'): SAGEConv((-1, -1), hidden_dim),
        }, aggr='sum')

        self.conv2 = HeteroConv({
            ('Tweet', 'uses', 'Hashtag'): SAGEConv((-1, -1), hidden_dim),
            ('Hashtag', 'rev_uses', 'Tweet'): SAGEConv((-1, -1), hidden_dim),
            ('Hashtag', 'related_to', 'Topic'): SAGEConv((-1, -1), hidden_dim),
            ('Topic', 'rev_related_to', 'Hashtag'): SAGEConv((-1, -1), hidden_dim),
            ('GitHubRepo', 'has_topic', 'Topic'): SAGEConv((-1, -1), hidden_dim),
            ('Topic', 'rev_has_topic', 'GitHubRepo'): SAGEConv((-1, -1), hidden_dim),
        }, aggr='sum')

    def forward(self, x_dict, edge_index_dict):
        x_dict = self.conv1(x_dict, edge_index_dict)
        x_dict = {k: x.relu() for k, x in x_dict.items()}
        x_dict = self.conv2(x_dict, edge_index_dict)
        return x_dict

class LinkPredictor(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        # Deeper network for better discrimination
        self.fc1 = nn.Linear(in_channels * 2, 256)
        self.dropout1 = nn.Dropout(0.2)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.1)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 1)

    def forward(self, z_twitter, z_github):
        # Enhanced architecture for better predictions
        x = torch.cat([z_twitter, z_github], dim=-1)

        # Layer 1 - simplified to avoid batch norm issues during inference
        x = self.fc1(x)
        x = torch.relu(x)
        if self.training:  # Only apply dropout during training
            x = self.dropout1(x)

        # Layer 2 - simplified to avoid batch norm issues during inference
        x = self.fc2(x)
        x = torch.relu(x)
        if self.training:  # Only apply dropout during training
            x = self.dropout2(x)

        # Layer 3
        x = torch.relu(self.fc3(x))

        # Output layer - return raw logits (no sigmoid)
        x = self.fc4(x)
        return x  # Return logits for better training dynamics

# Find pseudo-positive pairs from shared topic path
positive_pairs = set()
for htag, topic in edges_hashtag_topic:
    for repo, t2 in edges_repo_topic:
        if topic == t2:
            for tweet, h2 in edges_tweet_hashtag:
                if h2 == htag:
                    positive_pairs.add((tweet, repo))

positive_pairs = list(positive_pairs)

# Create equal number of negative pairs (1:1 ratio for balanced training)
num_neg = len(positive_pairs)  # Equal number of negatives
neg_tweet = np.random.randint(0, data['Tweet'].num_nodes, num_neg)
neg_repo = np.random.randint(0, data['GitHubRepo'].num_nodes, num_neg)
negative_pairs = list(zip(neg_tweet, neg_repo))

link_pred = LinkPredictor(in_channels=128)
model = HeteroGNN(hidden_dim=128)

# Use lower learning rate and add weight decay for regularization
optimizer = torch.optim.Adam(
    list(model.parameters()) + list(link_pred.parameters()),
    lr=0.001,  # Reduced learning rate
    weight_decay=1e-4  # L2 regularization
)
# Use BCEWithLogitsLoss since we're returning raw logits now
loss_fn = nn.BCEWithLogitsLoss()

for epoch in range(30):  # Increased to 30 epochs
    model.train()
    link_pred.train()
    optimizer.zero_grad()

    out = model(data.x_dict, data.edge_index_dict)
    z_tweet = out['Tweet']
    z_repo = out['GitHubRepo']

    # Positive scores
    t_idx = torch.tensor([t for t, _ in positive_pairs])
    r_idx = torch.tensor([r for _, r in positive_pairs])
    pos_scores = link_pred(z_tweet[t_idx], z_repo[r_idx])
    pos_labels = torch.ones_like(pos_scores)

    # Negative scores
    t_idx = torch.tensor([t for t, _ in negative_pairs])
    r_idx = torch.tensor([r for _, r in negative_pairs])
    neg_scores = link_pred(z_tweet[t_idx], z_repo[r_idx])
    neg_labels = torch.zeros_like(neg_scores)

    # Combine and compute loss
    scores = torch.cat([pos_scores, neg_scores], dim=0)
    labels = torch.cat([pos_labels, neg_labels], dim=0)

    loss = loss_fn(scores, labels)
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")

# ✅ After model.eval() and link_pred.eval()
model.eval()
link_pred.eval()

predicted_links = []
top_k = 10

# Map from GitHub repo index to URL
repo_index_to_url = github_df.set_index("Name")["URL"].to_dict()

# Map user_id → username
user_id_to_name = dict(zip(twitter_df["user_id"], twitter_df["username"]))

# Define keyword normalization mapping
keyword_aliases = {
    "ai": "artificial intelligence",
    "ml": "machine learning",
    "dl": "deep learning",
    "nlp": "natural language processing",
    "cv": "computer vision",
    "bigdata": "big data",
    "datasci": "data science",
    "crypto": "cryptography",
    "blockchain": "blockchain",
    "cloud": "cloud computing"
}

# Function to normalize keywords using aliases
def normalize_keywords(keywords):
    normalized = set()
    for word in keywords:
        word_lower = word.lower().strip()
        mapped = keyword_aliases.get(word_lower, word_lower)
        normalized.add(mapped)
    return normalized

with torch.no_grad():
    out = model(data.x_dict, data.edge_index_dict)
    z_tweet = out["Tweet"]
    z_repo = out["GitHubRepo"]

    unique_users = twitter_df['user_id'].unique()  # First 10 users

    for user_id in unique_users:
        username = user_id_to_name.get(user_id, f"user_{user_id}")

        user_tweets = user2tweets.get(user_id, [])
        if not user_tweets:
            continue

        tweet_indices = [tweet2id[tweet_id] for tweet_id in user_tweets if tweet_id in tweet2id]
        if not tweet_indices:
            continue

        user_tweet_embeddings = z_tweet[tweet_indices]
        avg_user_embedding = user_tweet_embeddings.mean(dim=0)

        # Find user hashtags
        user_hashtags = set()
        for tweet_id in user_tweets:
            tweet_row = twitter_df[twitter_df['unique_tweet_id'] == tweet_id]
            if not tweet_row.empty:
                user_hashtags.update(tweet_row.iloc[0]['hashtags'])

        # Normalize hashtags
        normalized_hashtags = normalize_keywords(user_hashtags)

        # Find related topics via normalized hashtags
        related_topics = {tag for tag in normalized_hashtags if tag in topic2id}

        # Find matching repos via topics
        matching_repos = set()
        for repo, topic_id in edges_repo_topic:
            topic_name = topics[topic_id].lower().strip()
            normalized_topic = keyword_aliases.get(topic_name, topic_name)
            if normalized_topic in related_topics:
                matching_repos.add(repo)

        if not matching_repos:
            continue

        matching_repos = list(matching_repos)
        repo_embeddings = z_repo[matching_repos]
        user_vec = avg_user_embedding.unsqueeze(0).repeat(repo_embeddings.size(0), 1)

        raw_logits = link_pred(user_vec, repo_embeddings).view(-1)
        pred_scores = torch.sigmoid(raw_logits)

        top_indices = torch.topk(pred_scores, k=min(top_k, len(pred_scores))).indices.tolist()

        for j in top_indices:
            repo_index = matching_repos[j]
            repo_name = repos[repo_index]
            if repo_name in repo_index_to_url:
                predicted_links.append({
                    "Username": username,
                    "RepoURL": repo_index_to_url[repo_name],
                    "PredictionScore": round(pred_scores[j].item(), 4)
                })

# Save and display final DataFrame
import pandas as pd
df = pd.DataFrame(predicted_links)
print(df.head(30))
df.to_csv("top_5_repo_predictions_detailed.csv", index=False)